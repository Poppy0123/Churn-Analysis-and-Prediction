{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ecb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the uploaded CSV file\n",
    "\n",
    "new_sales_data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f89cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning process: Checking and fixing the format of dates and numeric columns\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "new_sales_data['Date'] = pd.to_datetime(new_sales_data['Date'], errors='coerce')\n",
    "\n",
    "# Attempt to convert 'Sales Price' and 'Amount' columns to numeric format, coercing errors to NaN\n",
    "new_sales_data['Sales Price'] = pd.to_numeric(new_sales_data['Sales Price'].str.replace(' ', '').replace('', 'NaN'), errors='coerce')\n",
    "new_sales_data['Amount'] = pd.to_numeric(new_sales_data['Amount'].str.replace(' ', '').replace('', 'NaN'), errors='coerce')\n",
    "\n",
    "# Display data types and check for missing values in key columns\n",
    "data_types = new_sales_data.dtypes\n",
    "missing_values = new_sales_data.isnull().sum()\n",
    "\n",
    "data_types, missing_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify customers with any 'Shipping' in their transactions\n",
    "nonlocal_customers = new_sales_data[new_sales_data['Memo/Description'].str.contains('Shipping', na=False)]['Customer'].unique()\n",
    "\n",
    "# Label all transactions of these customers as 'nonlocal'\n",
    "new_sales_data['Customer Type'] = new_sales_data['Customer'].apply(lambda x: 'nonlocal' if x in nonlocal_customers else 'local')\n",
    "\n",
    "# Verify the labeling by displaying some entries of a customer marked as 'nonlocal'\n",
    "verification = new_sales_data[new_sales_data['Customer'] == nonlocal_customers[0]][['Customer', 'Memo/Description', 'Customer Type']].head()\n",
    "verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa57eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'Product/Service' with 'Unknown Product/Service'\n",
    "new_sales_data['Product/Service'].fillna('Unknown Product/Service', inplace=True)\n",
    "\n",
    "# Fill missing values in 'Sales Price' and 'Qty' with zeros\n",
    "new_sales_data['Sales Price'].fillna(0, inplace=True)\n",
    "new_sales_data['Qty'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill missing values in 'Memo/Description' with 'No Description'\n",
    "new_sales_data['Memo/Description'].fillna('No Description', inplace=True)\n",
    "\n",
    "# Remove rows where 'Amount' is missing\n",
    "new_sales_data.dropna(subset=['Amount'], inplace=True)\n",
    "\n",
    "# Confirm changes by showing the updated missing value count and a sample of the data\n",
    "updated_missing_values = new_sales_data.isnull().sum()\n",
    "updated_sample = new_sales_data.head()\n",
    "\n",
    "updated_missing_values, updated_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb52cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechecking the data types of key columns to ensure correct formatting\n",
    "data_types_recheck = new_sales_data.dtypes\n",
    "\n",
    "# Display data types to confirm everything is as expected\n",
    "data_types_recheck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'Date' is already in datetime format and sorted, if not we need to sort it\n",
    "new_sales_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Find the most recent transaction date in the data\n",
    "latest_date = new_sales_data['Date'].max()\n",
    "\n",
    "# Define the cutoff date for churn (6 months before the latest transaction date)\n",
    "cutoff_date = latest_date - pd.DateOffset(months=6)\n",
    "\n",
    "# Group by customer and find the date of the last purchase\n",
    "customer_last_purchase = new_sales_data.groupby('Customer')['Date'].max().reset_index()\n",
    "\n",
    "# Identify churned customers who have not purchased since the cutoff date\n",
    "customer_last_purchase['Churned'] = customer_last_purchase['Date'].apply(lambda x: 'Yes' if x <= cutoff_date else 'No')\n",
    "\n",
    "# Display the first few rows of the churned customers data\n",
    "churned_customers = customer_last_purchase.head()\n",
    "\n",
    "churned_customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall churn rate\n",
    "total_customers = customer_last_purchase.shape[0]\n",
    "churned_customers_count = customer_last_purchase[customer_last_purchase['Churned'] == 'Yes'].shape[0]\n",
    "churn_rate = (churned_customers_count / total_customers) * 100\n",
    "\n",
    "# Calculate churn rate by customer type\n",
    "churn_by_type = new_sales_data.groupby('Customer Type').apply(\n",
    "    lambda x: (x.groupby('Customer')['Date'].max() <= cutoff_date).mean() * 100\n",
    ").reset_index(name='Churn Rate')\n",
    "\n",
    "churn_rate, churn_by_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the churn data back to the main sales data to analyze behavior\n",
    "behavior_data = new_sales_data.merge(customer_last_purchase[['Customer', 'Churned']], on='Customer', how='left')\n",
    "\n",
    "# Calculate purchase frequency and average spending for churned and retained customers\n",
    "purchase_frequency = behavior_data.groupby(['Customer', 'Churned'])['Date'].count().reset_index().groupby('Churned')['Date'].mean()\n",
    "average_spending = behavior_data.groupby(['Customer', 'Churned'])['Amount'].sum().reset_index().groupby('Churned')['Amount'].mean()\n",
    "\n",
    "# Results\n",
    "purchase_frequency, average_spending\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize product names by removing 'VIP:' prefix\n",
    "behavior_data['Standardized Product'] = behavior_data['Product/Service'].str.replace('VIP:', '').str.strip()\n",
    "\n",
    "# Calculate the frequency of product purchases for churned and retained customers\n",
    "product_preferences = behavior_data.groupby(['Standardized Product', 'Churned'])['Num'].count().unstack(fill_value=0)\n",
    "product_preferences['Total'] = product_preferences.sum(axis=1)\n",
    "product_preferences.sort_values('Total', ascending=False, inplace=True)\n",
    "\n",
    "# Display the top products to see the difference in purchasing patterns\n",
    "top_product_preferences = product_preferences.head(10)\n",
    "top_product_preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d7e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract year and month from the 'Date' for grouping\n",
    "behavior_data['YearMonth'] = behavior_data['Date'].dt.to_period('M')\n",
    "\n",
    "# Calculate monthly churn rates\n",
    "monthly_churn_data = behavior_data.groupby(['YearMonth', 'Customer'])['Churned'].max().reset_index()\n",
    "monthly_churn_rates = monthly_churn_data.groupby('YearMonth')['Churned'].apply(lambda x: (x == 'Yes').mean() * 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Churned' from Yes/No to 1/0 for correlation analysis\n",
    "behavior_data['Churned_Flag'] = (behavior_data['Churned'] == 'Yes').astype(int)\n",
    "\n",
    "# Calculate correlations of 'Churned_Flag' with 'Qty' (as a proxy for frequency) and 'Amount' (as a proxy for spending)\n",
    "correlation_data = behavior_data[['Churned_Flag', 'Qty', 'Amount']]\n",
    "correlations = correlation_data.corr()\n",
    "\n",
    "correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9a56e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for logistic regression\n",
    "# Encoding categorical variables\n",
    "encoder = LabelEncoder()\n",
    "behavior_data['Customer Type Encoded'] = encoder.fit_transform(behavior_data['Customer Type'])\n",
    "behavior_data['Product Encoded'] = encoder.fit_transform(behavior_data['Standardized Product'])\n",
    "\n",
    "# Selecting features for the model\n",
    "X = behavior_data[['Qty', 'Amount', 'Customer Type Encoded', 'Product Encoded']]\n",
    "y = behavior_data['Churned_Flag']\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and performance evaluation\n",
    "predictions = model.predict(X_test)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848bb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the time between purchases for each customer\n",
    "behavior_data['Previous Purchase Date'] = behavior_data.groupby('Customer')['Date'].shift(1)\n",
    "behavior_data['Days Between Purchases'] = (behavior_data['Date'] - behavior_data['Previous Purchase Date']).dt.days\n",
    "\n",
    "# Average time between purchases per customer\n",
    "avg_time_between_purchases = behavior_data.groupby('Customer')['Days Between Purchases'].mean()\n",
    "\n",
    "# Count of unique product types purchased by each customer\n",
    "product_variety = behavior_data.groupby('Customer')['Standardized Product'].nunique()\n",
    "\n",
    "# Merge these new features into the main dataset\n",
    "customer_features = behavior_data[['Customer', 'Churned_Flag']].drop_duplicates().set_index('Customer')\n",
    "customer_features['Avg Days Between Purchases'] = avg_time_between_purchases\n",
    "customer_features['Product Variety'] = product_variety\n",
    "\n",
    "customer_features.reset_index(inplace=True)\n",
    "\n",
    "# Display a sample of the new features\n",
    "customer_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new binary feature indicating whether a discount was applied (VIP)\n",
    "behavior_data['Discount Applied'] = behavior_data['Product/Service'].apply(lambda x: 1 if 'VIP:' in str(x) else 0)\n",
    "\n",
    "# Update customer_features with the discount information\n",
    "discount_info = behavior_data.groupby('Customer')['Discount Applied'].max()\n",
    "customer_features['Discount Applied'] = customer_features['Customer'].map(discount_info)\n",
    "\n",
    "# Fill missing values in 'Avg Days Between Purchases' with the median of the available values\n",
    "median_days_between_purchases = customer_features['Avg Days Between Purchases'].median()\n",
    "customer_features['Avg Days Between Purchases'].fillna(median_days_between_purchases, inplace=True)\n",
    "\n",
    "# Update logistic regression model including new features\n",
    "X = customer_features[['Avg Days Between Purchases', 'Product Variety', 'Discount Applied']]\n",
    "y = customer_features['Churned_Flag']\n",
    "\n",
    "# Splitting data into training and testing sets again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic regression model\n",
    "updated_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and performance evaluation\n",
    "updated_predictions = updated_model.predict(X_test)\n",
    "updated_report = classification_report(y_test, updated_predictions)\n",
    "\n",
    "updated_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdaebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Configure the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_report = classification_report(y_test, rf_predictions)\n",
    "\n",
    "rf_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importances from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "features = X.columns\n",
    "\n",
    "# Create a DataFrame to visualize the feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setting up parameter grid to tune the model\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',  # can be adjusted to other metrics\n",
    "                           verbose=1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train Random Forest model with the optimal parameters\n",
    "tuned_rf_model = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=10, random_state=42)\n",
    "tuned_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model with the tuned parameters\n",
    "tuned_rf_predictions = tuned_rf_model.predict(X_test)\n",
    "tuned_rf_report = classification_report(y_test, tuned_rf_predictions)\n",
    "\n",
    "tuned_rf_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96e767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Set up the Gradient Boosting Classifier\n",
    "gbm_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "gbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "gbm_predictions = gbm_model.predict(X_test)\n",
    "gbm_report = classification_report(y_test, gbm_predictions)\n",
    "\n",
    "gbm_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define the MLPClassifier\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64, 32), activation='relu', solver='adam', max_iter=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluate the model\n",
    "mlp_predictions = mlp_model.predict(X_test)\n",
    "mlp_report = classification_report(y_test, mlp_predictions)\n",
    "\n",
    "mlp_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ce5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95751fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a7263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e8d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
